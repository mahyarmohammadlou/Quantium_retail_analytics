{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T17:03:30.155615Z",
     "start_time": "2025-10-29T17:03:12.812438Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "tx = pd.read_excel(\"data/raw/QVI_transaction_data.xlsx\")\n",
    "pb = pd.read_excel(\"data/raw/QVI_purchase_behaviour.xlsx\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:04:14.591957Z",
     "start_time": "2025-10-29T17:04:14.566043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic info\n",
    "print(\"=== TRANSACTION DATA ===\")\n",
    "print(tx.shape)\n",
    "print(tx.info())\n",
    "print(\"\\nSample rows:\\n\", tx.head(3))\n",
    "\n",
    "print(\"\\n=== PURCHASE BEHAVIOUR DATA ===\")\n",
    "print(pb.shape)\n",
    "print(pb.info())\n",
    "print(\"\\nSample rows:\\n\", pb.head(3))\n"
   ],
   "id": "d30f572cf519b6ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRANSACTION DATA ===\n",
      "(264836, 8)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 264836 entries, 0 to 264835\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   DATE            264836 non-null  int64  \n",
      " 1   STORE_NBR       264836 non-null  int64  \n",
      " 2   LYLTY_CARD_NBR  264836 non-null  int64  \n",
      " 3   TXN_ID          264836 non-null  int64  \n",
      " 4   PROD_NBR        264836 non-null  int64  \n",
      " 5   PROD_NAME       264836 non-null  object \n",
      " 6   PROD_QTY        264836 non-null  int64  \n",
      " 7   TOT_SALES       264836 non-null  float64\n",
      "dtypes: float64(1), int64(6), object(1)\n",
      "memory usage: 16.2+ MB\n",
      "None\n",
      "\n",
      "Sample rows:\n",
      "     DATE  STORE_NBR  LYLTY_CARD_NBR  TXN_ID  PROD_NBR  \\\n",
      "0  43390          1            1000       1         5   \n",
      "1  43599          1            1307     348        66   \n",
      "2  43605          1            1343     383        61   \n",
      "\n",
      "                                PROD_NAME  PROD_QTY  TOT_SALES  \n",
      "0  Natural Chip        Compny SeaSalt175g         2        6.0  \n",
      "1                CCs Nacho Cheese    175g         3        6.3  \n",
      "2  Smiths Crinkle Cut  Chips Chicken 170g         2        2.9  \n",
      "\n",
      "=== PURCHASE BEHAVIOUR DATA ===\n",
      "(72637, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72637 entries, 0 to 72636\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   LYLTY_CARD_NBR    72637 non-null  int64 \n",
      " 1   LIFESTAGE         72637 non-null  object\n",
      " 2   PREMIUM_CUSTOMER  72637 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "\n",
      "Sample rows:\n",
      "    LYLTY_CARD_NBR              LIFESTAGE PREMIUM_CUSTOMER\n",
      "0            1000  YOUNG SINGLES/COUPLES          Premium\n",
      "1            1002  YOUNG SINGLES/COUPLES       Mainstream\n",
      "2            1003         YOUNG FAMILIES           Budget\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:04:25.025244Z",
     "start_time": "2025-10-29T17:04:24.983161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  Quick numeric overview\n",
    "print(\"\\n=== Numeric Summary (Transactions) ===\")\n",
    "print(tx.describe().T[[\"mean\", \"std\", \"min\", \"max\"]].head(10))\n"
   ],
   "id": "789d8769be737f89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Numeric Summary (Transactions) ===\n",
      "                         mean           std      min        max\n",
      "DATE             43464.036260    105.389282  43282.0    43646.0\n",
      "STORE_NBR          135.080110     76.784180      1.0      272.0\n",
      "LYLTY_CARD_NBR  135549.476404  80579.978022   1000.0  2373711.0\n",
      "TXN_ID          135158.310815  78133.026026      1.0  2415841.0\n",
      "PROD_NBR            56.583157     32.826638      1.0      114.0\n",
      "PROD_QTY             1.907309      0.643654      1.0      200.0\n",
      "TOT_SALES            7.304200      3.083226      1.5      650.0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:04:57.414569Z",
     "start_time": "2025-10-29T17:04:42.727640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Quality Check: DATE ---\n",
    "print(tx[\"DATE\"].head(10).tolist())\n",
    "raw2 = pd.read_excel(\"data/raw/QVI_transaction_data.xlsx\")\n",
    "print(type(raw2.loc[0,\"DATE\"]), raw2.loc[0,\"DATE\"])\n"
   ],
   "id": "4c95e6227020f495",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43390, 43599, 43605, 43329, 43330, 43604, 43601, 43601, 43332, 43330]\n",
      "<class 'numpy.int64'> 43390\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:06:30.887588Z",
     "start_time": "2025-10-29T17:05:34.049515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Force to numeric if the column was auto-parsed to datetime improperly\n",
    "s = pd.to_numeric(tx[\"DATE\"], errors=\"coerce\")\n",
    "\n",
    "# Helper to convert mixed encodings safely:\n",
    "# - Excel serial days   ~ 30000..50000 (â‰ˆ 1982â€“2137; Quantium ~ 43k)\n",
    "# - Unix seconds        ~ 1.0e9..2.0e9\n",
    "# - Unix milliseconds   ~ 1.0e12..2.0e12\n",
    "# Otherwise -> NaT\n",
    "def to_datetime_mixed(v):\n",
    "    if pd.isna(v):\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        v = float(v)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "    if 30000 <= v <= 50000:\n",
    "        return pd.to_datetime(\"1899-12-30\") + pd.to_timedelta(int(v), unit=\"D\")\n",
    "    if 1_000_000_000 <= v <= 2_000_000_000:\n",
    "        return pd.to_datetime(int(v), unit=\"s\")\n",
    "    if 1_000_000_000_000 <= v <= 2_000_000_000_000:\n",
    "        return pd.to_datetime(int(v), unit=\"ms\")\n",
    "    return pd.NaT  # out-of-range or noise\n",
    "\n",
    "tx[\"DATE\"] = s.apply(to_datetime_mixed)\n",
    "\n",
    "# Drop invalid/missing dates\n",
    "tx = tx.dropna(subset=[\"DATE\"]).copy()\n",
    "\n",
    "# Normalize to date (remove time) and quick sanity\n",
    "tx[\"DATE\"] = tx[\"DATE\"].dt.floor(\"D\")\n",
    "print(\"Date range:\", tx[\"DATE\"].min().date(), \"â†’\", tx[\"DATE\"].max().date())\n",
    "print(\"Rows with valid DATE:\", len(tx))\n",
    "print(\"\\n Sample dates:\", sorted(tx[\"DATE\"].sample(min(10, len(tx)), random_state=42).tolist()))\n"
   ],
   "id": "1de0fe9c6df78382",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2018-07-01 â†’ 2019-06-30\n",
      "Rows with valid DATE: 264836\n",
      "\n",
      " Sample dates: [Timestamp('2018-07-14 00:00:00'), Timestamp('2018-07-21 00:00:00'), Timestamp('2018-09-15 00:00:00'), Timestamp('2018-09-22 00:00:00'), Timestamp('2018-10-18 00:00:00'), Timestamp('2019-02-19 00:00:00'), Timestamp('2019-03-09 00:00:00'), Timestamp('2019-04-02 00:00:00'), Timestamp('2019-04-09 00:00:00'), Timestamp('2019-06-12 00:00:00')]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:06:54.924793Z",
     "start_time": "2025-10-29T17:06:54.913784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Quality Check: STORE_NBR (Store Number) ---\n",
    "# Check datatype and basic structure\n",
    "print(\" Column dtype:\", tx[\"STORE_NBR\"].dtype)\n",
    "print(\" Unique store count:\", tx[\"STORE_NBR\"].nunique())\n",
    "print(\" Sample store numbers:\", sorted(tx['STORE_NBR'].unique()[:10]))\n",
    "\n",
    "# Check for missing or invalid values\n",
    "null_count = tx[\"STORE_NBR\"].isna().sum()\n",
    "print(f\"\\nMissing STORE_NBR values: {null_count} ({null_count / len(tx) * 100:.3f}%)\")\n",
    "\n",
    "# Check numeric validity (should be integer, positive)\n",
    "invalid_values = tx.loc[\n",
    "    (tx[\"STORE_NBR\"] < 0) | (tx[\"STORE_NBR\"] % 1 != 0), \"STORE_NBR\"\n",
    "]\n",
    "if not invalid_values.empty:\n",
    "    print(\"\\n Invalid STORE_NBR values detected (negative or non-integer):\")\n",
    "    print(invalid_values.unique()[:10])\n",
    "else:\n",
    "    print(\"\\n All STORE_NBR values are positive integers.\")\n",
    "\n",
    "# Range sanity check\n",
    "min_store = tx[\"STORE_NBR\"].min()\n",
    "max_store = tx[\"STORE_NBR\"].max()\n",
    "print(f\"\\n STORE_NBR range: {min_store} â†’ {max_store}\")"
   ],
   "id": "d7702ce36d099cc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Column dtype: int64\n",
      " Unique store count: 272\n",
      " Sample store numbers: [np.int64(1), np.int64(2), np.int64(4), np.int64(5), np.int64(7), np.int64(8), np.int64(9), np.int64(13), np.int64(19), np.int64(20)]\n",
      "\n",
      "Missing STORE_NBR values: 0 (0.000%)\n",
      "\n",
      " All STORE_NBR values are positive integers.\n",
      "\n",
      " STORE_NBR range: 1 â†’ 272\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:07:32.792594Z",
     "start_time": "2025-10-29T17:07:32.763192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Quality Check: LYLTY_CARD_NBR (Customer ID) ---\n",
    "# Check datatype and missing values\n",
    "print(\" Column dtype:\", tx[\"LYLTY_CARD_NBR\"].dtype)\n",
    "null_count = tx[\"LYLTY_CARD_NBR\"].isna().sum()\n",
    "print(f\" Missing LYLTY_CARD_NBR values: {null_count} ({null_count / len(tx) * 100:.3f}%)\")\n",
    "\n",
    "# Drop nulls if any\n",
    "if null_count > 0:\n",
    "    tx = tx.dropna(subset=[\"LYLTY_CARD_NBR\"]).copy()\n",
    "    print(\" Null values removed from LYLTY_CARD_NBR.\")\n",
    "else:\n",
    "    print(\" No nulls to remove in LYLTY_CARD_NBR.\")\n",
    "\n",
    "# Ensure numeric and positive\n",
    "tx[\"LYLTY_CARD_NBR\"] = pd.to_numeric(tx[\"LYLTY_CARD_NBR\"], errors=\"coerce\")\n",
    "invalid_ids = tx.loc[tx[\"LYLTY_CARD_NBR\"] <= 0, \"LYLTY_CARD_NBR\"]\n",
    "if not invalid_ids.empty:\n",
    "    print(\"\\n Invalid (non-positive) customer IDs detected:\")\n",
    "    print(invalid_ids.unique()[:10])\n",
    "else:\n",
    "    print(\"\\n All customer IDs are positive integers.\")\n",
    "\n",
    "# Check for duplicate transactions by same customer on same date\n",
    "dupes = tx[tx.duplicated(subset=[\"LYLTY_CARD_NBR\", \"DATE\"], keep=False)]\n",
    "\n",
    "if not dupes.empty:\n",
    "    print(f\"\\n Potential duplicate transactions found: {len(dupes)} rows\")\n",
    "    print(\"Sample duplicate records:\")\n",
    "    print(dupes.head(5)[[\"LYLTY_CARD_NBR\", \"DATE\", \"TXN_ID\", \"TOT_SALES\"]])\n",
    "else:\n",
    "    print(\"\\n No duplicate customer transactions found on the same date.\")"
   ],
   "id": "544d2d140c3473f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Column dtype: int64\n",
      " Missing LYLTY_CARD_NBR values: 0 (0.000%)\n",
      " No nulls to remove in LYLTY_CARD_NBR.\n",
      "\n",
      " All customer IDs are positive integers.\n",
      "\n",
      " Potential duplicate transactions found: 3410 rows\n",
      "Sample duplicate records:\n",
      "     LYLTY_CARD_NBR       DATE  TXN_ID  TOT_SALES\n",
      "41            55073 2019-05-20   48887       3.25\n",
      "42            55073 2019-05-20   48887       4.60\n",
      "376            7364 2019-01-10    7739       8.80\n",
      "377            7364 2019-01-10    7739      11.40\n",
      "418           12301 2018-10-18   10982       8.80\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:08:02.019829Z",
     "start_time": "2025-10-29T17:08:02.009425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if duplicates also share the same TXN_ID\n",
    "same_txn = dupes.groupby([\"LYLTY_CARD_NBR\", \"DATE\"])[\"TXN_ID\"].nunique().reset_index()\n",
    "same_txn_issue = same_txn[same_txn[\"TXN_ID\"] > 1]\n",
    "\n",
    "print(f\" Duplicates with different TXN_IDs: {len(same_txn_issue)}\")\n",
    "\n",
    "if len(same_txn_issue) == 0:\n",
    "    print(\" All duplicates belong to the same TXN_ID â†’ multi-product purchases (not true duplicates).\")\n",
    "else:\n",
    "    print(\" Some duplicates have different TXN_IDs â†’ potential data duplication.\")\n",
    "    print(same_txn_issue.head(10))\n"
   ],
   "id": "655a79f08160055",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Duplicates with different TXN_IDs: 3\n",
      " Some duplicates have different TXN_IDs â†’ potential data duplication.\n",
      "     LYLTY_CARD_NBR       DATE  TXN_ID\n",
      "985          155014 2019-02-22       3\n",
      "986          155035 2018-09-17       2\n",
      "989          155092 2018-12-16       2\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:10:03.235481Z",
     "start_time": "2025-10-29T17:09:37.866911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Quality Check: PROD_NAME (Product Name) ---\n",
    "\n",
    "# Compile regex patterns\n",
    "re_pack = re.compile(r\"(\\d+)\\s*[Gg]\\b\")  # e.g. 175g, 210G\n",
    "\n",
    "#  multi-word brands first to avoid partial matches\n",
    "multi_brands = [\n",
    "    r\"RED ROCK DELI\", r\"NATURAL CHIP CO\", r\"GRAIN WAVES\", r\"OLD EL PASO\"\n",
    "]\n",
    "re_multi_brand = re.compile(rf\"^({'|'.join(multi_brands)})\\b\", re.I)\n",
    "re_first_token = re.compile(r\"^([A-Za-z']+)\\b\")\n",
    "\n",
    "# --- expanded variant line patterns ---\n",
    "re_variant = re.compile(\n",
    "    r\"\\b(\"\n",
    "    r\"CRINKLE\\s+CUT|THIN(LY)?\\s+CUT|RIDGE(S)?|RIDGE\\s+CUT|WAVES?|WAVY|\"\n",
    "    r\"CORN\\s+CHIPS?|TORTILLA|SENSATIONS|PAPADUMS|CRIS?PS?\"\n",
    "    r\")\\b\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "# --- brand normalization map ---\n",
    "brand_alias = {\n",
    "    \"RRD\": \"Red Rock Deli\",\n",
    "    \"RED ROCK\": \"Red Rock Deli\",\n",
    "    \"RED ROCK DELI\": \"Red Rock Deli\",\n",
    "    \"WW\": \"Woolworths\",\n",
    "    \"WOOLWORTHS\": \"Woolworths\",\n",
    "    \"DORITO\": \"Doritos\",\n",
    "    \"DORITOS\": \"Doritos\",\n",
    "    \"INFZNS\": \"Infuzions\",\n",
    "    \"INFUZIONS\": \"Infuzions\",\n",
    "    \"SMITH\": \"Smiths\",\n",
    "    \"SMITHS\": \"Smiths\",\n",
    "    \"NATURAL CHIP\": \"Natural Chip Co\",\n",
    "    \"NATURAL CHIP CO\": \"Natural Chip Co\",\n",
    "    \"GRAIN WAVES\": \"Grain Waves\",\n",
    "}\n",
    "\n",
    "# --- helpers ---\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def normalize_brand(b: str) -> str:\n",
    "    if not b:\n",
    "        return b\n",
    "    b_up = b.upper().strip()\n",
    "    for k, v in brand_alias.items():\n",
    "        if b_up.startswith(k):\n",
    "            return v\n",
    "    return b.title().strip()\n",
    "\n",
    "def extract_parts(name: str):\n",
    "    \"\"\"Return BRAND, VARIANT_LINE, FLAVOR, PACK_SIZE from PROD_NAME.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return pd.Series([None, None, None, None],\n",
    "                         index=[\"BRAND\",\"VARIANT_LINE\",\"FLAVOR\",\"PACK_SIZE\"])\n",
    "\n",
    "    raw = normalize_spaces(str(name))\n",
    "\n",
    "    # ---- PACK_SIZE ----\n",
    "    pack_size = None\n",
    "    remainder = raw\n",
    "    m_pack = re_pack.search(raw)\n",
    "    if m_pack:\n",
    "        pack_size = float(m_pack.group(1))\n",
    "        remainder = normalize_spaces(re_pack.sub(\"\", remainder))\n",
    "\n",
    "    # ---- BRAND ----\n",
    "    brand = None\n",
    "    rem_after_brand = remainder\n",
    "\n",
    "    m_mb = re_multi_brand.search(remainder.upper())\n",
    "    if m_mb:\n",
    "        start, end = m_mb.span()\n",
    "        brand = remainder[start:end].strip()\n",
    "        rem_after_brand = remainder[end:].strip()\n",
    "    else:\n",
    "        m1 = re_first_token.search(remainder)\n",
    "        if m1:\n",
    "            brand = m1.group(1).strip()\n",
    "            rem_after_brand = remainder[m1.end():].strip()\n",
    "\n",
    "    brand = normalize_brand(brand)\n",
    "\n",
    "    # ---- VARIANT_LINE ----\n",
    "    variant_line = None\n",
    "    rem_after_variant = rem_after_brand\n",
    "    if rem_after_brand:\n",
    "        m_v = re_variant.search(rem_after_brand)\n",
    "        if m_v:\n",
    "            variant_line = m_v.group(1).strip()\n",
    "            rem_after_variant = normalize_spaces(\n",
    "                re_variant.sub(\"\", rem_after_brand, count=1)\n",
    "            )\n",
    "\n",
    "    # ---- FLAVOR ----\n",
    "    flavor = rem_after_variant if rem_after_variant else None\n",
    "    if flavor:\n",
    "        flavor = normalize_spaces(flavor)\n",
    "        if not flavor.isupper():\n",
    "            flavor = flavor.title()\n",
    "\n",
    "    return pd.Series([brand, variant_line, flavor, pack_size],\n",
    "                     index=[\"BRAND\",\"VARIANT_LINE\",\"FLAVOR\",\"PACK_SIZE\"])\n",
    "\n",
    "\n",
    "# Apply to dataframe\n",
    "parsed = tx[\"PROD_NAME\"].apply(extract_parts)\n",
    "tx[[\"BRAND\",\"VARIANT_LINE\",\"FLAVOR\",\"PACK_SIZE\"]] = parsed\n",
    "\n",
    "# Quick diagnostics\n",
    "print(\" Created columns: BRAND, VARIANT_LINE, FLAVOR, PACK_SIZE\")\n",
    "\n",
    "print(\"\\n Null counts:\")\n",
    "print(tx[[\"BRAND\",\"VARIANT_LINE\",\"FLAVOR\",\"PACK_SIZE\"]].isna().sum())\n",
    "\n",
    "print(\"\\n Top 15 brands:\")\n",
    "print(tx[\"BRAND\"].value_counts().head(15).to_string())\n",
    "\n",
    "print(\"\\n Top variant lines:\")\n",
    "print(tx[\"VARIANT_LINE\"].dropna().value_counts().head(15).to_string())\n",
    "\n",
    "print(\"\\n PACK_SIZE basic stats:\")\n",
    "print(tx[\"PACK_SIZE\"].describe())\n"
   ],
   "id": "b7d5c1ec1c515b41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Created columns: BRAND, VARIANT_LINE, FLAVOR, PACK_SIZE\n",
      "\n",
      " Null counts:\n",
      "BRAND                0\n",
      "VARIANT_LINE    188977\n",
      "FLAVOR               0\n",
      "PACK_SIZE            0\n",
      "dtype: int64\n",
      "\n",
      " Top 15 brands:\n",
      "BRAND\n",
      "Kettle           41288\n",
      "Smiths           31823\n",
      "Doritos          28147\n",
      "Pringles         25102\n",
      "Red Rock Deli    17779\n",
      "Woolworths       14757\n",
      "Infuzions        14201\n",
      "Thins            14075\n",
      "Cobs              9693\n",
      "Tostitos          9471\n",
      "Twisties          9454\n",
      "Old El Paso       9324\n",
      "Tyrrells          6442\n",
      "Grain Waves       6272\n",
      "Cheezels          4603\n",
      "\n",
      " Top variant lines:\n",
      "VARIANT_LINE\n",
      "Crinkle Cut    17621\n",
      "Crisps         12607\n",
      "Corn Chips     12502\n",
      "Tortilla        9580\n",
      "Sensations      9429\n",
      "Corn Chip       6376\n",
      "Thinly Cut      3133\n",
      "Crips           3104\n",
      "Papadums        1507\n",
      "\n",
      " PACK_SIZE basic stats:\n",
      "count    264836.000000\n",
      "mean        182.427004\n",
      "std          64.327196\n",
      "min          70.000000\n",
      "25%         150.000000\n",
      "50%         170.000000\n",
      "75%         175.000000\n",
      "max         380.000000\n",
      "Name: PACK_SIZE, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:10:13.075559Z",
     "start_time": "2025-10-29T17:10:13.056559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Quality Check: QUANTITY (PROD_QTY) ---\n",
    "\n",
    "# Summary statistics\n",
    "print(\" Summary statistics for QUANTITY:\")\n",
    "print(tx[\"PROD_QTY\"].describe())\n",
    "\n",
    "# Check for missing, zero, or negative values\n",
    "print(\"\\n Missing, zero, and negative value check:\")\n",
    "print(f\"Null values: {tx['PROD_QTY'].isna().sum()}\")\n",
    "print(f\"Zero values: {(tx['PROD_QTY'] == 0).sum()}\")\n",
    "print(f\"Negative values: {(tx['PROD_QTY'] < 0).sum()}\")\n",
    "\n",
    "# Frequency of unique values to understand purchase size patterns\n",
    "print(\"\\n Frequency of common purchase quantities:\")\n",
    "print(tx[\"PROD_QTY\"].value_counts().sort_index())\n",
    "\n",
    "#  Identify potential outliers (very large purchase quantities)\n",
    "print(\"\\n Potential outliers (QUANTITY > 10):\")\n",
    "print(tx[tx[\"PROD_QTY\"] > 10][[\"PROD_NAME\", \"PROD_QTY\"]].head(10))\n"
   ],
   "id": "c197f8eaa55c23cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summary statistics for QUANTITY:\n",
      "count    264836.000000\n",
      "mean          1.907309\n",
      "std           0.643654\n",
      "min           1.000000\n",
      "25%           2.000000\n",
      "50%           2.000000\n",
      "75%           2.000000\n",
      "max         200.000000\n",
      "Name: PROD_QTY, dtype: float64\n",
      "\n",
      " Missing, zero, and negative value check:\n",
      "Null values: 0\n",
      "Zero values: 0\n",
      "Negative values: 0\n",
      "\n",
      " Frequency of common purchase quantities:\n",
      "PROD_QTY\n",
      "1       27518\n",
      "2      236039\n",
      "3         430\n",
      "4         397\n",
      "5         450\n",
      "200         2\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Potential outliers (QUANTITY > 10):\n",
      "                              PROD_NAME  PROD_QTY\n",
      "69762  Dorito Corn Chp     Supreme 380g       200\n",
      "69763  Dorito Corn Chp     Supreme 380g       200\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:10:21.860042Z",
     "start_time": "2025-10-29T17:10:21.759640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove unrealistic outliers in quantity (e.g., 200 packs)\n",
    "tx = tx[tx[\"PROD_QTY\"] <= 10].copy()\n",
    "\n",
    "print(\" Cleaned QUANTITY column.\")\n",
    "print(tx[\"PROD_QTY\"].describe())\n",
    "print(tx[\"PROD_QTY\"].value_counts().sort_index())\n"
   ],
   "id": "a28b806966699fc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaned QUANTITY column.\n",
      "count    264834.000000\n",
      "mean          1.905813\n",
      "std           0.343436\n",
      "min           1.000000\n",
      "25%           2.000000\n",
      "50%           2.000000\n",
      "75%           2.000000\n",
      "max           5.000000\n",
      "Name: PROD_QTY, dtype: float64\n",
      "PROD_QTY\n",
      "1     27518\n",
      "2    236039\n",
      "3       430\n",
      "4       397\n",
      "5       450\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:10:35.436967Z",
     "start_time": "2025-10-29T17:10:35.419079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Quality Check: TOT_SALES (Total Sales Amount) ---\n",
    "\n",
    "#  Summary statistics\n",
    "print(\" Summary statistics for TOT_SALES:\")\n",
    "print(tx[\"TOT_SALES\"].describe())\n",
    "\n",
    "#  Check for missing, zero, or negative values\n",
    "print(\"\\n Missing, zero, and negative value check:\")\n",
    "print(f\"Null values: {tx['TOT_SALES'].isna().sum()}\")\n",
    "print(f\"Zero values: {(tx['TOT_SALES'] == 0).sum()}\")\n",
    "print(f\"Negative values: {(tx['TOT_SALES'] < 0).sum()}\")\n",
    "\n",
    "#  Frequency of common sales amounts (rounded for better readability)\n",
    "print(\"\\n Frequency of common transaction amounts:\")\n",
    "print(tx[\"TOT_SALES\"].round(2).value_counts().sort_index().head(20))\n",
    "\n",
    "#  Identify potential outliers (very large total sales values)\n",
    "print(\"\\n Potential outliers (TOT_SALES > 30):\")\n",
    "print(tx[tx[\"TOT_SALES\"] > 30][[\"PROD_NAME\", \"PROD_QTY\", \"TOT_SALES\"]].head(10))\n"
   ],
   "id": "2a23636f0f6a71d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summary statistics for TOT_SALES:\n",
      "count    264834.000000\n",
      "mean          7.299346\n",
      "std           2.527241\n",
      "min           1.500000\n",
      "25%           5.400000\n",
      "50%           7.400000\n",
      "75%           9.200000\n",
      "max          29.500000\n",
      "Name: TOT_SALES, dtype: float64\n",
      "\n",
      " Missing, zero, and negative value check:\n",
      "Null values: 0\n",
      "Zero values: 0\n",
      "Negative values: 0\n",
      "\n",
      " Frequency of common transaction amounts:\n",
      "TOT_SALES\n",
      "1.50     354\n",
      "1.70     708\n",
      "1.80     175\n",
      "1.90     930\n",
      "2.10     732\n",
      "2.30     186\n",
      "2.40     182\n",
      "2.60    1287\n",
      "2.70    1231\n",
      "2.80     189\n",
      "2.90     886\n",
      "3.00    5434\n",
      "3.10     188\n",
      "3.25      88\n",
      "3.30    1539\n",
      "3.40    5146\n",
      "3.60    1963\n",
      "3.70    2459\n",
      "3.80    8585\n",
      "3.90     601\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Potential outliers (TOT_SALES > 30):\n",
      "Empty DataFrame\n",
      "Columns: [PROD_NAME, PROD_QTY, TOT_SALES]\n",
      "Index: []\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:10:57.448134Z",
     "start_time": "2025-10-29T17:10:57.404389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Quality Check: LYLTY_CARD_NBR (Customer ID) ---\n",
    "\n",
    "# Summary info\n",
    "print(\" Summary statistics for LYLTY_CARD_NBR:\")\n",
    "print(pb[\"LYLTY_CARD_NBR\"].describe())\n",
    "\n",
    "# Check for missing or duplicate IDs\n",
    "print(\"\\n Missing and duplicate check:\")\n",
    "print(f\"Null values: {pb['LYLTY_CARD_NBR'].isna().sum()}\")\n",
    "print(f\"Duplicate values: {pb['LYLTY_CARD_NBR'].duplicated().sum()}\")\n",
    "\n",
    "# Inspect min and max to ensure numeric consistency\n",
    "print(\"\\n Range of customer IDs:\")\n",
    "print(f\"Min ID: {pb['LYLTY_CARD_NBR'].min()}, Max ID: {pb['LYLTY_CARD_NBR'].max()}\")\n",
    "\n",
    "# Quick integrity cross-check with transaction data\n",
    "common_ids = set(tx[\"LYLTY_CARD_NBR\"]).intersection(set(pb[\"LYLTY_CARD_NBR\"]))\n",
    "print(f\"\\n Common customer IDs between datasets: {len(common_ids)}\")\n",
    "print(f\"Unique IDs in transactions only: {tx['LYLTY_CARD_NBR'].nunique() - len(common_ids)}\")\n",
    "print(f\"Unique IDs in purchase behaviour only: {pb['LYLTY_CARD_NBR'].nunique() - len(common_ids)}\")\n"
   ],
   "id": "b743b8e642c68bf8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summary statistics for LYLTY_CARD_NBR:\n",
      "count    7.263700e+04\n",
      "mean     1.361859e+05\n",
      "std      8.989293e+04\n",
      "min      1.000000e+03\n",
      "25%      6.620200e+04\n",
      "50%      1.340400e+05\n",
      "75%      2.033750e+05\n",
      "max      2.373711e+06\n",
      "Name: LYLTY_CARD_NBR, dtype: float64\n",
      "\n",
      " Missing and duplicate check:\n",
      "Null values: 0\n",
      "Duplicate values: 0\n",
      "\n",
      " Range of customer IDs:\n",
      "Min ID: 1000, Max ID: 2373711\n",
      "\n",
      " Common customer IDs between datasets: 72636\n",
      "Unique IDs in transactions only: 0\n",
      "Unique IDs in purchase behaviour only: 1\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:11:15.643251Z",
     "start_time": "2025-10-29T17:11:15.620243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Quality Check: LIFESTAGE (Customer Life Stage) ---\n",
    "\n",
    "# Summary info\n",
    "print(\" Summary of LIFESTAGE column:\")\n",
    "print(pb[\"LIFESTAGE\"].describe())\n",
    "\n",
    "# Check for missing or blank values\n",
    "print(\"\\n Missing or blank value check:\")\n",
    "print(f\"Null values: {pb['LIFESTAGE'].isna().sum()}\")\n",
    "print(f\"Blank strings: {(pb['LIFESTAGE'].str.strip() == '').sum()}\")\n",
    "\n",
    "# Unique category listing and counts\n",
    "print(\"\\n Unique life stage categories and counts:\")\n",
    "print(pb[\"LIFESTAGE\"].value_counts().sort_index())\n",
    "\n",
    "#  Detect potential typos or inconsistent formatting\n",
    "print(\"\\n Potential inconsistent capitalization or spacing:\")\n",
    "print(pb[\"LIFESTAGE\"].unique())\n"
   ],
   "id": "b2b0b0e1986c809e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summary of LIFESTAGE column:\n",
      "count        72637\n",
      "unique           7\n",
      "top       RETIREES\n",
      "freq         14805\n",
      "Name: LIFESTAGE, dtype: object\n",
      "\n",
      " Missing or blank value check:\n",
      "Null values: 0\n",
      "Blank strings: 0\n",
      "\n",
      " Unique life stage categories and counts:\n",
      "LIFESTAGE\n",
      "MIDAGE SINGLES/COUPLES     7275\n",
      "NEW FAMILIES               2549\n",
      "OLDER FAMILIES             9780\n",
      "OLDER SINGLES/COUPLES     14609\n",
      "RETIREES                  14805\n",
      "YOUNG FAMILIES             9178\n",
      "YOUNG SINGLES/COUPLES     14441\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Potential inconsistent capitalization or spacing:\n",
      "['YOUNG SINGLES/COUPLES' 'YOUNG FAMILIES' 'OLDER SINGLES/COUPLES'\n",
      " 'MIDAGE SINGLES/COUPLES' 'NEW FAMILIES' 'OLDER FAMILIES' 'RETIREES']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:11:34.819213Z",
     "start_time": "2025-10-29T17:11:34.794836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Data Quality Check: PREMIUM_CUSTOMER (Customer Price Segment) ---\n",
    "\n",
    "# Summary info\n",
    "print(\" Summary of PREMIUM_CUSTOMER column:\")\n",
    "print(pb[\"PREMIUM_CUSTOMER\"].describe())\n",
    "\n",
    "# Check for missing or blank values\n",
    "print(\"\\n Missing or blank value check:\")\n",
    "print(f\"Null values: {pb['PREMIUM_CUSTOMER'].isna().sum()}\")\n",
    "print(f\"Blank strings: {(pb['PREMIUM_CUSTOMER'].str.strip() == '').sum()}\")\n",
    "\n",
    "# Unique category listing and counts\n",
    "print(\"\\n Unique customer type categories and counts:\")\n",
    "print(pb[\"PREMIUM_CUSTOMER\"].value_counts().sort_index())\n",
    "\n",
    "# Detect potential inconsistent capitalization or spacing\n",
    "print(\"\\n Potential inconsistent capitalization or spacing:\")\n",
    "print(pb[\"PREMIUM_CUSTOMER\"].unique())\n"
   ],
   "id": "5588db1fd2466143",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Summary of PREMIUM_CUSTOMER column:\n",
      "count          72637\n",
      "unique             3\n",
      "top       Mainstream\n",
      "freq           29245\n",
      "Name: PREMIUM_CUSTOMER, dtype: object\n",
      "\n",
      " Missing or blank value check:\n",
      "Null values: 0\n",
      "Blank strings: 0\n",
      "\n",
      " Unique customer type categories and counts:\n",
      "PREMIUM_CUSTOMER\n",
      "Budget        24470\n",
      "Mainstream    29245\n",
      "Premium       18922\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Potential inconsistent capitalization or spacing:\n",
      "['Premium' 'Mainstream' 'Budget']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:14:13.979698Z",
     "start_time": "2025-10-29T17:14:12.937341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Save cleaned datasets before merging ---\n",
    "\n",
    "import os\n",
    "\n",
    "# Define output directory (create if not exists)\n",
    "output_dir = \"data/cleaned\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save cleaned transaction dataset\n",
    "tx.to_csv(f\"{output_dir}/QVI_transaction_cleaned.csv\", index=False)\n",
    "print(\" Saved cleaned transaction data â†’ data/cleaned/QVI_transaction_cleaned.csv\")\n",
    "\n",
    "# Save cleaned purchase behaviour dataset\n",
    "pb.to_csv(f\"{output_dir}/QVI_purchase_behaviour_cleaned.csv\", index=False)\n",
    "print(\" Saved cleaned purchase behaviour data â†’ data/cleaned/QVI_purchase_behaviour_cleaned.csv\")\n",
    "\n",
    "# Optional quick check: confirm file shapes and columns\n",
    "print(\"\\n Transaction Data Shape:\", tx.shape)\n",
    "print(\" Columns:\", list(tx.columns))\n",
    "print(\"\\n Purchase Behaviour Shape:\", pb.shape)\n",
    "print(\" Columns:\", list(pb.columns))\n"
   ],
   "id": "17cd9f49ac7caa2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved cleaned transaction data â†’ data/cleaned/QVI_transaction_cleaned.csv\n",
      "âœ… Saved cleaned purchase behaviour data â†’ data/cleaned/QVI_purchase_behaviour_cleaned.csv\n",
      "\n",
      "ðŸ“¦ Transaction Data Shape: (264834, 12)\n",
      "ðŸ§¾ Columns: ['DATE', 'STORE_NBR', 'LYLTY_CARD_NBR', 'TXN_ID', 'PROD_NBR', 'PROD_NAME', 'PROD_QTY', 'TOT_SALES', 'BRAND', 'VARIANT_LINE', 'FLAVOR', 'PACK_SIZE']\n",
      "\n",
      "ðŸ‘¤ Purchase Behaviour Shape: (72637, 3)\n",
      "ðŸ§  Columns: ['LYLTY_CARD_NBR', 'LIFESTAGE', 'PREMIUM_CUSTOMER']\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T17:23:28.806122Z",
     "start_time": "2025-10-29T17:23:27.555392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Phase 3: Merge & Save Final Combined Dataset ---\n",
    "\n",
    "# Load cleaned datasets\n",
    "tx_clean = pd.read_csv(\"data/cleaned/QVI_transaction_cleaned.csv\")\n",
    "pb_clean = pd.read_csv(\"data/cleaned/QVI_purchase_behaviour_cleaned.csv\")\n",
    "\n",
    "# Merge datasets on customer ID\n",
    "merged = pd.merge(tx_clean, pb_clean, on=\"LYLTY_CARD_NBR\", how=\"inner\")\n",
    "\n",
    "# Check merge result\n",
    "print(\" Merge successful!\")\n",
    "print(f\" Merged dataset shape: {merged.shape}\")\n",
    "print(f\" Columns combined: {len(tx_clean.columns)} + {len(pb_clean.columns) - 1} â†’ {len(merged.columns)} columns\")\n",
    "\n",
    "# Check for duplicates (each TXN_ID Ã— PROD_NBR should be unique)\n",
    "duplicates = merged.duplicated(subset=[\"TXN_ID\", \"PROD_NBR\"]).sum()\n",
    "print(f\" Duplicate transaction-product pairs: {duplicates}\")\n",
    "\n",
    "# Save merged dataset to final folder\n",
    "output_dir = \"data/final\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "merged.to_csv(f\"{output_dir}/QVI_merged_dataset.csv\", index=False)\n",
    "\n",
    "print(\"\\n Final merged dataset saved as â†’ data/final/QVI_merged_dataset.csv\")\n",
    "\n",
    "# Optional preview\n",
    "print(\"\\n Sample merged rows:\")\n",
    "print(merged.sample(5, random_state=42).to_string(index=False))\n"
   ],
   "id": "7d9a53cd51f830b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merge successful!\n",
      " Merged dataset shape: (264834, 14)\n",
      " Columns combined: 12 + 2 â†’ 14 columns\n",
      " Duplicate transaction-product pairs: 1\n",
      "\n",
      " Final merged dataset saved as â†’ data/final/QVI_merged_dataset.csv\n",
      "\n",
      " Sample merged rows:\n",
      "      DATE  STORE_NBR  LYLTY_CARD_NBR  TXN_ID  PROD_NBR                                PROD_NAME  PROD_QTY  TOT_SALES     BRAND VARIANT_LINE                    FLAVOR  PACK_SIZE             LIFESTAGE PREMIUM_CUSTOMER\n",
      "2019-05-23          7            7222    7214        88         Kettle Honey Soy    Chicken 175g         2       10.8    Kettle          NaN         Honey Soy Chicken      175.0 OLDER SINGLES/COUPLES           Budget\n",
      "2019-03-15         10           10112    9712       104 Infuzions Thai SweetChili PotatoMix 110g         2        7.6 Infuzions          NaN Thai Sweetchili Potatomix      110.0        YOUNG FAMILIES          Premium\n",
      "2019-06-19        166          166089  167582        90        Tostitos Smoked     Chipotle 175g         2        8.8  Tostitos          NaN           Smoked Chipotle      175.0        YOUNG FAMILIES          Premium\n",
      "2019-03-07         41           41484   38469        68      Pringles Chicken    Salt Crips 134g         2        7.4  Pringles        Crips              Chicken Salt      134.0 YOUNG SINGLES/COUPLES       Mainstream\n",
      "2019-03-13        136          136111  138496        36                       Kettle Chilli 175g         2       10.8    Kettle          NaN                    Chilli      175.0              RETIREES          Premium\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "287a38cf82348c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
